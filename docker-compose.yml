version: "3.9"
services:
  # ==========================================================================
  # API Server (FastAPI)
  # ==========================================================================
  api:
    build: ./backend
    ports:
      - "8000:8000"
    env_file:
      - ./.env
    environment:
      - DATABASE_URL=postgresql://postgres:postgres@db:5432/olala
      # SLM 기본값 (env_file이 우선 적용됨)
      - SLM_BASE_URL=http://ollama:11434/v1
    depends_on:
      - db
      - ollama

  # ==========================================================================
  # PostgreSQL Database
  # ==========================================================================
  db:
    image: postgres:15
    environment:
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: olala
    ports:
      - "5432:5432"
    volumes:
      - olala_pg:/var/lib/postgresql/data

  # ==========================================================================
  # Ollama - Local LLM Server (OpenAI-compatible)
  # ==========================================================================
  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama

  # ==========================================================================
  # SLM2 Test Runner (Stage 6-8 테스트 전용)
  # 사용법: docker compose run --rm slm2-test --case 1
  # ==========================================================================
  slm2-test:
    image: python:3.11-slim
    volumes:
      - ./backend:/work
    working_dir: /work
    env_file:
      - ./.env
    environment:
      - SLM_BASE_URL=http://ollama:11434/v1
      - PYTHONPATH=/work
    depends_on:
      - ollama
    entrypoint: ["sh", "-c"]
    command: ["pip install -q -r requirements.txt && python -m tests.demo_slm2_pipeline $@", "--"]
    profiles:
      - test

  # ==========================================================================
  # Optional: vLLM container for GPU-accelerated SLM serving
  # GPU 환경에서만 사용 가능. 주석 해제 후 HF_TOKEN 설정 필요
  # ==========================================================================
  # vllm:
  #   image: vllm/vllm-openai:latest
  #   ports:
  #     - "8001:8001"
  #   environment:
  #     - HF_TOKEN=replace_me
  #   volumes:
  #     - ./mlops/models:/root/.cache/huggingface
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]
  #   command: >
  #     vllm serve Qwen/Qwen2.5-7B-Instruct-AWQ
  #     --host 0.0.0.0
  #     --port 8001
  #     --served-model-name slm
  #     --dtype auto
  #     --max-model-len 4096
  #     --quantization awq
  #     --api-key local-slm-key
  #     --trust-remote-code

volumes:
  olala_pg:
  ollama_data:
